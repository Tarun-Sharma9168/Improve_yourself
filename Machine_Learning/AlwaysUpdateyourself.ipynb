{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All About I learned !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First day Of Learning...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.   -0.98 -0.96 -0.94 -0.92 -0.9  -0.88 -0.86 -0.84 -0.82 -0.8  -0.78\n",
      " -0.76 -0.74 -0.72 -0.7  -0.68 -0.66 -0.64 -0.62 -0.6  -0.58 -0.56 -0.54\n",
      " -0.52 -0.5  -0.48 -0.46 -0.44 -0.42 -0.4  -0.38 -0.36 -0.34 -0.32 -0.3\n",
      " -0.28 -0.26 -0.24 -0.22 -0.2  -0.18 -0.16 -0.14 -0.12 -0.1  -0.08 -0.06\n",
      " -0.04 -0.02  0.    0.02  0.04  0.06  0.08  0.1   0.12  0.14  0.16  0.18\n",
      "  0.2   0.22  0.24  0.26  0.28  0.3   0.32  0.34  0.36  0.38  0.4   0.42\n",
      "  0.44  0.46  0.48  0.5   0.52  0.54  0.56  0.58  0.6   0.62  0.64  0.66\n",
      "  0.68  0.7   0.72  0.74  0.76  0.78  0.8   0.82  0.84  0.86  0.88  0.9\n",
      "  0.92  0.94  0.96  0.98  1.  ]\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "#It is generaring the 101 points between a certain a range\n",
    "import numpy as np\n",
    "trx=np.linspace(-1,1,101)\n",
    "print(trx)\n",
    "print(len(trx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 -0.98 -0.96 -0.94 -0.92 -0.9 -0.88 -0.86 -0.84 -0.8200000000000001 -0.8 -0.78 -0.76 -0.74 -0.72 -0.7 -0.6799999999999999 -0.6599999999999999 -0.64 -0.62 -0.6 -0.5800000000000001 -0.56 -0.54 -0.52 -0.5 -0.48 -0.45999999999999996 -0.43999999999999995 -0.42000000000000004 -0.4 -0.38 -0.36 -0.33999999999999997 -0.31999999999999995 -0.29999999999999993 -0.28 -0.26 -0.24 -0.21999999999999997 -0.19999999999999996 -0.17999999999999994 -0.16000000000000003 -0.14 -0.12 -0.09999999999999998 -0.07999999999999996 -0.05999999999999994 -0.040000000000000036 -0.020000000000000018 0.0 0.020000000000000018 0.040000000000000036 0.06000000000000005 0.08000000000000007 0.10000000000000009 0.1200000000000001 0.14000000000000012 0.15999999999999992 0.17999999999999994 0.19999999999999996 0.21999999999999997 0.24 0.26 0.28 0.30000000000000004 0.32000000000000006 0.3400000000000001 0.3600000000000001 0.3800000000000001 0.40000000000000013 0.41999999999999993 0.43999999999999995 0.45999999999999996 0.48 0.5 0.52 0.54 0.56 0.5800000000000001 0.6000000000000001 0.6200000000000001 0.6400000000000001 0.6600000000000001 0.6799999999999999 0.7 0.72 0.74 0.76 0.78 0.8 0.8200000000000001 0.8400000000000001 0.8600000000000001 0.8800000000000001 0.9000000000000001 0.9199999999999999 0.94 0.96 0.98 1.0\n"
     ]
    }
   ],
   "source": [
    "#Expanding and print the original value using this very nice first time seeing \n",
    "#that is pevious one is round off but this is original value....\n",
    "print(*trx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second new thing today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0062592845125607945"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randn means if you want to fill the values by standard normal distribution that we generally assume the data to be\n",
    "#when we take random values\n",
    "pp=np.random.randn(*trx.shape)\n",
    "\n",
    "\n",
    "#if we check the mean if approx 0\n",
    "pp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9185360448923392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and if we talk about the standard deviation it is 1\n",
    "pp.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Basic prepreprocessing which is always useful\n",
    "which contains the reshaping ,converting the type and also the normalization of the input\n",
    "and the conversion of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load MNIST dataset\n",
    "\n",
    "#Loading of the predefined datasets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape,\" \",y_train.shape)\n",
    "print(X_test.shape,\" \",y_test.shape)\n",
    "X_train = X_train.reshape(60000, 784)#make it into 2 dimesnional\n",
    "\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')#images always be in float32 using astype \n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalizing the data\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#np_utils to convert it into categorical_values using to_categorical\n",
    "Y_Train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_Test = np_utils.to_categorical(y_test, nb_classes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips to compile and then compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Logistic regression model\n",
    "model = Sequential()\n",
    "#you know the output dim no need to tell input shape \n",
    "\n",
    "\n",
    "model.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\n",
    "#compiling with the sgd as an optimizer and the learning rate small using loss function as categorical crossentropy\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Logistic regression model\\nmodel = Sequential()\\n#you know the output dim no need to tell input shape \\nmodel.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\\n#compiling with the sgd as an optimizer and the learning rate small using loss function as categorical crossentropy\\nmodel.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\\n\\nmodel.summary()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Logistic regression model\n",
    "model = Sequential()\n",
    "#you know the output dim no need to tell input shape \n",
    "model.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\n",
    "#compiling with the sgd as an optimizer and the learning rate small using loss function as categorical crossentropy\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rho paramter used with RMSprop -Rho is a hyper-parameter which attenuates the influence of past gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Deep Multilayer Perceptron model\n",
    "model = Sequential()\n",
    "\n",
    "#First Layer\n",
    "model.add(Dense(output_dim=625, input_dim=784, init='normal'))\n",
    "#Activation layer explicitly\n",
    "model.add(Activation('relu'))\n",
    "#Dropout layer explicitly\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "#Second Layer\n",
    "model.add(Dense(output_dim=625, input_dim=625, init='normal'))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "#Last and the output Layer\n",
    "model.add(Dense(output_dim=10, input_dim=625, init='normal'))\n",
    "#Classifier Softmax at the end\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Compilation of the model now with the new Paramter rho and everything other is same\n",
    "#These are the very standard values of the learning rate and the rho value\n",
    "model.compile(optimizer=RMSprop(lr=0.001, rho=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Seeing the summary statistics \n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are specific dropout rate for the conv_layer also\n",
    "mentioned by prob_drop_conv =0.2\n",
    "for the hidden layers  it is prob_drop_hidden\n",
    "pool_size=(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Layer network with all the basic steps and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Convolutional model\n",
    "init_weights='normal'\n",
    "model = Sequential()\n",
    "\n",
    "# conv1 layer\n",
    "# there are so many important parameters in the conv2d layerhere here border_mode is new others are we mentioned before\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu', input_shape=input_shape, init=init_weights))\n",
    "\n",
    "#Adding the MaxPooling2D layer with the pool_size and the movement describer of that window which is 2,2 very important\n",
    "#No of convolution which is 32 above these are also important as every conv layer is responsible for very important features\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "\n",
    "#Adding the dropout to the conv2d layer with prob_drop_conv rate\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# conv2 layer\n",
    "#Same configuration layer\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', init=init_weights))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "# conv3 layer\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', init=init_weights))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), border_mode='same'))\n",
    "\n",
    "\n",
    "#Now here comes the time to flatten the layer\n",
    "model.add(Flatten())\n",
    "#At the end also the dropout is there\n",
    "model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "\n",
    "#Now time comes for the fully connected layers\n",
    "# fc1 layer\n",
    "model.add(Dense(625, activation='relu', init=init_weights))\n",
    "#How important is the Dropout while doing Deep Learning code just see\n",
    "model.add(Dropout(prob_drop_hidden))\n",
    "\n",
    "# fc2 layer\n",
    "# Output Layer \n",
    "model.add(Dense(10, activation='softmax', init=init_weights))\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding of the random noise in the neural network is very common so we see how we do it in mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "# Load MNIST Dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "#converting the data type and the normalization \n",
    "x_train = x_train.astype('float32') / 255.\n",
    "\n",
    "#Same with the test set\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "#it is nothing but the same thing like we did it previously the way is change that is np.prod these \n",
    "#numpy functions are optimized that is all nothing more than that\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will test this what output we get from this \n",
    "# loc and scale is nothing but the mean and standard deviation which is 0 and 1 in Gaussian normal distribution\n",
    "noise=np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "len(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function normal:\n",
      "\n",
      "normal(...) method of numpy.random.mtrand.RandomState instance\n",
      "    normal(loc=0.0, scale=1.0, size=None)\n",
      "    \n",
      "    Draw random samples from a normal (Gaussian) distribution.\n",
      "    \n",
      "    The probability density function of the normal distribution, first\n",
      "    derived by De Moivre and 200 years later by both Gauss and Laplace\n",
      "    independently [2]_, is often called the bell curve because of\n",
      "    its characteristic shape (see the example below).\n",
      "    \n",
      "    The normal distributions occurs often in nature.  For example, it\n",
      "    describes the commonly occurring distribution of samples influenced\n",
      "    by a large number of tiny, random disturbances, each with its own\n",
      "    unique distribution [2]_.\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``normal`` method of a ``default_rng()``\n",
      "        instance instead; see `random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    loc : float or array_like of floats\n",
      "        Mean (\"centre\") of the distribution.\n",
      "    scale : float or array_like of floats\n",
      "        Standard deviation (spread or \"width\") of the distribution. Must be\n",
      "        non-negative.\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n",
      "        a single value is returned if ``loc`` and ``scale`` are both scalars.\n",
      "        Otherwise, ``np.broadcast(loc, scale).size`` samples are drawn.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray or scalar\n",
      "        Drawn samples from the parameterized normal distribution.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scipy.stats.norm : probability density function, distribution or\n",
      "        cumulative density function, etc.\n",
      "    Generator.normal: which should be used for new code.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The probability density for the Gaussian distribution is\n",
      "    \n",
      "    .. math:: p(x) = \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 }}\n",
      "                     e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2} },\n",
      "    \n",
      "    where :math:`\\mu` is the mean and :math:`\\sigma` the standard\n",
      "    deviation. The square of the standard deviation, :math:`\\sigma^2`,\n",
      "    is called the variance.\n",
      "    \n",
      "    The function has its peak at the mean, and its \"spread\" increases with\n",
      "    the standard deviation (the function reaches 0.607 times its maximum at\n",
      "    :math:`x + \\sigma` and :math:`x - \\sigma` [2]_).  This implies that\n",
      "    normal is more likely to return samples lying close to the mean, rather\n",
      "    than those far away.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Wikipedia, \"Normal distribution\",\n",
      "           https://en.wikipedia.org/wiki/Normal_distribution\n",
      "    .. [2] P. R. Peebles Jr., \"Central Limit Theorem\" in \"Probability,\n",
      "           Random Variables and Random Signal Principles\", 4th ed., 2001,\n",
      "           pp. 51, 51, 125.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Draw samples from the distribution:\n",
      "    \n",
      "    >>> mu, sigma = 0, 0.1 # mean and standard deviation\n",
      "    >>> s = np.random.normal(mu, sigma, 1000)\n",
      "    \n",
      "    Verify the mean and the variance:\n",
      "    \n",
      "    >>> abs(mu - np.mean(s))\n",
      "    0.0  # may vary\n",
      "    \n",
      "    >>> abs(sigma - np.std(s, ddof=1))\n",
      "    0.1  # may vary\n",
      "    \n",
      "    Display the histogram of the samples, along with\n",
      "    the probability density function:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> count, bins, ignored = plt.hist(s, 30, density=True)\n",
      "    >>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
      "    ...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
      "    ...          linewidth=2, color='r')\n",
      "    >>> plt.show()\n",
      "    \n",
      "    Two-by-four array of samples from N(3, 6.25):\n",
      "    \n",
      "    >>> np.random.normal(3, 2.5, size=(2, 4))\n",
      "    array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n",
      "           [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random noise\n",
    "# We  studied already adding random noise also do a work for as the regularizer in the neural network and it \n",
    "# is very generally used \n",
    "\n",
    "x_train_noisy = x_train + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "\n",
    "x_test_noisy = x_test + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "print(x_train_noisy.shape)\n",
    "print(x_test_noisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are AutoEncoders?????\n",
    "#https://towardsdatascience.com/autoencoders-bits-and-bytes-of-deep-learning-eaba376f23ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to show the image from the raw values that are there in matrices \n",
    "#just use the matplotlib which is there very important and very beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#this is important as now we can able to see the denoising the images that is the main work of the AutoEncoders\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Decode test images \n",
    "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    #plt.imshow function to show the images\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
